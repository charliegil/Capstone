{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "TyemDLPdJeU4"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Generate random symbols as input\n",
    "def generate_symbols(input_length, M):\n",
    "    return torch.randint(0, M, (input_length,))  # generate (batch_size) random symbols between 0 and M - 1\n",
    "\n",
    "# Create sliding windows from input data\n",
    "def create_windows(data, window_size, step_size):\n",
    "    num_windows = (len(data) - window_size) // step_size + 1\n",
    "    windows = torch.stack([\n",
    "        data[i:i + window_size]  # Extract rows for each window\n",
    "        for i in range(0, num_windows * step_size, step_size)\n",
    "    ])\n",
    "\n",
    "    return windows\n",
    "\n",
    "def plot_two_lines(x1, y1, x2, y2, title, x_label, y_label, label1=\"Line 1\", label2=\"Line 2\"):\n",
    "    \"\"\"\n",
    "    Creates an interactive Plotly plot for two solid lines over the same x-axis.\n",
    "\n",
    "    Parameters:\n",
    "    - x1, y1: Data for the first line\n",
    "    - x2, y2: Data for the second line\n",
    "    - title: Plot title\n",
    "    - x_label: Label for the x-axis\n",
    "    - y_label: Label for the y-axis\n",
    "    - label1: Legend name for the first line (default: \"Line 1\")\n",
    "    - label2: Legend name for the second line (default: \"Line 2\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an interactive figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add first line trace (solid blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x1, y=y1,\n",
    "        mode='lines', name=label1,\n",
    "        line=dict(color='blue', width=2)  # Solid line, blue color\n",
    "    ))\n",
    "\n",
    "    # Add second line trace (solid red)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x2, y=y2,\n",
    "        mode='lines', name=label2,\n",
    "        line=dict(color='red', width=2)  # Solid line, red color\n",
    "    ))\n",
    "\n",
    "    # Configure layout for better visibility\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_label,\n",
    "        yaxis_title=y_label,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x\",  # Enables hover tool on x-axis\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "def plot_losses(losses):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(losses, label=\"Training Loss\", color=\"blue\", alpha=0.7)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZjAcV_7A52C"
   },
   "source": [
    "# REMEMBER TO IMPORT: model_weights_windowed.pth, best_params_windowed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdpaC4p2mbm0"
   },
   "source": [
    "# MODIFIED TRAINING (INTEGRATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9pbnCZk1vHpu",
    "outputId": "24872bab-3661-45e7-a7b9-becb045cc929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Loss: 5.4213\n",
      "Epoch [2/1000] - Loss: 5.5457\n",
      "Epoch [3/1000] - Loss: 5.4821\n",
      "Epoch [4/1000] - Loss: 5.6221\n",
      "Epoch [5/1000] - Loss: 5.6502\n",
      "Epoch [6/1000] - Loss: 5.6511\n",
      "Epoch [7/1000] - Loss: 5.5749\n",
      "Epoch [8/1000] - Loss: 5.7153\n",
      "Epoch [9/1000] - Loss: 5.4875\n",
      "Epoch [10/1000] - Loss: 5.5445\n",
      "Epoch [11/1000] - Loss: 5.5112\n",
      "Epoch [12/1000] - Loss: 5.6185\n",
      "Epoch [13/1000] - Loss: 5.8456\n",
      "Epoch [14/1000] - Loss: 5.4847\n",
      "Epoch [15/1000] - Loss: 5.5412\n",
      "Epoch [16/1000] - Loss: 5.7027\n",
      "Epoch [17/1000] - Loss: 5.7369\n",
      "Epoch [18/1000] - Loss: 5.6246\n",
      "Epoch [19/1000] - Loss: 5.5105\n",
      "Epoch [20/1000] - Loss: 5.4628\n",
      "Epoch [21/1000] - Loss: 5.7663\n",
      "Epoch [22/1000] - Loss: 5.5619\n",
      "Epoch [23/1000] - Loss: 5.5153\n",
      "Epoch [24/1000] - Loss: 5.5062\n",
      "Epoch [25/1000] - Loss: 5.5759\n",
      "Epoch [26/1000] - Loss: 5.0101\n",
      "Epoch [27/1000] - Loss: 5.7332\n",
      "Epoch [28/1000] - Loss: 5.5114\n",
      "Epoch [29/1000] - Loss: 5.7486\n",
      "Epoch [30/1000] - Loss: 5.8488\n",
      "Epoch [31/1000] - Loss: 5.7868\n",
      "Epoch [32/1000] - Loss: 5.7908\n",
      "Epoch [33/1000] - Loss: 5.7003\n",
      "Epoch [34/1000] - Loss: 5.7325\n",
      "Epoch [35/1000] - Loss: 5.5877\n",
      "Epoch [36/1000] - Loss: 5.6128\n",
      "Epoch [37/1000] - Loss: 5.5162\n",
      "Epoch [38/1000] - Loss: 5.6877\n",
      "Epoch [39/1000] - Loss: 4.9398\n",
      "Epoch [40/1000] - Loss: 5.7014\n",
      "Epoch [41/1000] - Loss: 5.6400\n",
      "Epoch [42/1000] - Loss: 5.6382\n",
      "Epoch [43/1000] - Loss: 5.8040\n",
      "Epoch [44/1000] - Loss: 5.7122\n",
      "Epoch [45/1000] - Loss: 4.9831\n",
      "Epoch [46/1000] - Loss: 5.6136\n",
      "Epoch [47/1000] - Loss: 5.7524\n",
      "Epoch [48/1000] - Loss: 5.8369\n",
      "Epoch [49/1000] - Loss: 5.6397\n",
      "Epoch [50/1000] - Loss: 5.7396\n",
      "Epoch [51/1000] - Loss: 5.7229\n",
      "Epoch [52/1000] - Loss: 4.7592\n",
      "Epoch [53/1000] - Loss: 5.7373\n",
      "Epoch [54/1000] - Loss: 4.7151\n",
      "Epoch [55/1000] - Loss: 5.9679\n",
      "Epoch [56/1000] - Loss: 5.6651\n",
      "Epoch [57/1000] - Loss: 5.7944\n",
      "Epoch [58/1000] - Loss: 5.8428\n",
      "Epoch [59/1000] - Loss: 4.7851\n",
      "Epoch [60/1000] - Loss: 5.7579\n",
      "Epoch [61/1000] - Loss: 5.8708\n",
      "Epoch [62/1000] - Loss: 5.7744\n",
      "Epoch [63/1000] - Loss: 5.8491\n",
      "Epoch [64/1000] - Loss: 6.1214\n",
      "Epoch [65/1000] - Loss: 5.7500\n",
      "Epoch [66/1000] - Loss: 5.9187\n",
      "Epoch [67/1000] - Loss: 4.4848\n",
      "Epoch [68/1000] - Loss: 6.0526\n",
      "Epoch [69/1000] - Loss: 5.3821\n",
      "Epoch [70/1000] - Loss: 5.7563\n",
      "Epoch [71/1000] - Loss: 4.5702\n",
      "Epoch [72/1000] - Loss: 5.7554\n",
      "Epoch [73/1000] - Loss: 5.8130\n",
      "Epoch [74/1000] - Loss: 5.9421\n",
      "Epoch [75/1000] - Loss: 5.9938\n",
      "Epoch [76/1000] - Loss: 5.0223\n",
      "Epoch [77/1000] - Loss: 6.0946\n",
      "Epoch [78/1000] - Loss: 5.9581\n",
      "Epoch [79/1000] - Loss: 5.9489\n",
      "Epoch [80/1000] - Loss: 4.8404\n",
      "Epoch [81/1000] - Loss: 4.6602\n",
      "Epoch [82/1000] - Loss: 6.1701\n",
      "Epoch [83/1000] - Loss: 5.9645\n",
      "Epoch [84/1000] - Loss: 5.9744\n",
      "Epoch [85/1000] - Loss: 6.0902\n",
      "Epoch [86/1000] - Loss: 5.9003\n",
      "Epoch [87/1000] - Loss: 6.0770\n",
      "Epoch [88/1000] - Loss: 5.7537\n",
      "Epoch [89/1000] - Loss: 5.8272\n",
      "Epoch [90/1000] - Loss: 4.9035\n",
      "Epoch [91/1000] - Loss: 5.6093\n",
      "Epoch [92/1000] - Loss: 5.4816\n",
      "Epoch [93/1000] - Loss: 5.9813\n",
      "Epoch [94/1000] - Loss: 6.1398\n",
      "Epoch [95/1000] - Loss: 5.8560\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(decoded, target_symbol\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m--> 118\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loader(weights_filepath, params_filepath):\n",
    "    weights = torch.load(weights_filepath, weights_only=True, map_location=torch.device('cpu'))\n",
    "    with open(params_filepath, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "    return weights, params\n",
    "\n",
    "# Load model weights and parameters\n",
    "weights, params = loader(\n",
    "    r\"C:\\Users\\tomha\\Documents\\Capstone D2 - PC\\Constellation\\256QAM and 256 PSK\\Basic\\model_weights.pth\",\n",
    "    r\"C:\\Users\\tomha\\Documents\\Capstone D2 - PC\\Constellation\\256QAM and 256 PSK\\Basic\\best_params.json\"\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "M = 256\n",
    "n = 2\n",
    "window_size = 512\n",
    "epochs = 1000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define Transmitter\n",
    "class Transmitter(nn.Module):\n",
    "    def __init__(self, M, n):\n",
    "        super(Transmitter, self).__init__()\n",
    "        self.fc1 = nn.Linear(M, 8 * M)\n",
    "        self.fc2 = nn.Linear(8 * M, 8 * M) # try wider model deeper\n",
    "        self.fc3 = nn.Linear(8 * M, n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define Channel\n",
    "# Define the wider model\n",
    "class FiberOpticFNN0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN0, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),  # Dropout for regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class IdentityChannel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# Define Receiver\n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, n, M):\n",
    "        super(Receiver, self).__init__()\n",
    "        self.fc1 = nn.Linear(n, 8 * M)\n",
    "        self.fc2 = nn.Linear(8 * M, 8 * M)  # try wider model -> deeper\n",
    "        self.fc3 = nn.Linear(8 * M, M)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "transmitter = Transmitter(M, n)\n",
    "\n",
    "channel = FiberOpticFNN0(2 * window_size, params[\"hidden_dim\"], n, params[\"dropout\"])\n",
    "channel.load_state_dict(weights[\"model weights\"], strict=False)\n",
    "# channel = IdentityChannel();\n",
    "\n",
    "receiver = Receiver(n, M)\n",
    "\n",
    "for param in transmitter.parameters():\n",
    "    assert param.requires_grad\n",
    "for param in channel.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in receiver.parameters():\n",
    "    assert param.requires_grad\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(transmitter.parameters()) + list(receiver.parameters()), lr=learning_rate)\n",
    "\n",
    "# Training loop with batching\n",
    "transmitter.train()\n",
    "channel.eval()\n",
    "receiver.train()\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  symbols = torch.randint(0, M, (window_size,))\n",
    "  target_symbol = symbols[window_size // 2]\n",
    "  symbols_one_hot = F.one_hot(symbols, num_classes=M).float()\n",
    "\n",
    "  transmitted = transmitter(symbols_one_hot)\n",
    "\n",
    "  transmitted_flat = transmitted.view(1, -1)\n",
    "  received = channel(transmitted_flat)\n",
    "\n",
    "  decoded = receiver(received)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss = criterion(decoded, target_symbol.unsqueeze(0))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
    "  training_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    " identity matrix --> transmitter --> identity channel --> reciever --> identity matrix\n",
    "from model import TransmitterRecieverModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "input_size = 16\n",
    "hidden_size = 8\n",
    "channel_size = 2\n",
    "channel_func = lambda x: x\n",
    "\n",
    "model = TransmitterRecieverModel(input_size,hidden_size,channel_size,channel_func)\n",
    "\n",
    "input_data = np.array([range(input_size) for _ in range(1)]).reshape(-1)\n",
    "input_data_one_hot = np.eye(input_size)[input_data]\n",
    "\n",
    "model.train_model(torch.tensor(input_data_one_hot,dtype=torch.float32),torch.tensor(input_data,dtype=torch.long), learning_rate=0.1,epochs=1000)\n",
    "\n",
    "output = model.evaluate(torch.tensor(input_data_one_hot,dtype=torch.float32))\n",
    "print(\"Input Data: \")\n",
    "print(input_data)\n",
    "print(\"Output Data: \")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x2 and 1024x320)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mrange\u001b[39m(input_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m input_data_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(input_size)[input_data]\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(torch\u001b[38;5;241m.\u001b[39mtensor(input_data_one_hot,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Data: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[105], line 40\u001b[0m, in \u001b[0;36mTransmitterRecieverModel.train_model\u001b[1;34m(self, x, y, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 40\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred,y)\n\u001b[0;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[105], line 21\u001b[0m, in \u001b[0;36mTransmitterRecieverModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m transmitter_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransmitter_fc1(x))\n\u001b[0;32m     19\u001b[0m transmitter_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransmitter_fc2(transmitter_output)\n\u001b[1;32m---> 21\u001b[0m channel_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransmitter_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m reciever_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreciever_fc1(channel_output))\n\u001b[0;32m     24\u001b[0m reciever_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreciever_fc2(reciever_output)\n",
      "Cell \u001b[1;32mIn[106], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      9\u001b[0m channel \u001b[38;5;241m=\u001b[39m FiberOpticFNN0(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m window_size, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m], n, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m channel\u001b[38;5;241m.\u001b[39mload_state_dict(weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m channel_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mchannel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m TransmitterRecieverModel(input_size,hidden_size,channel_size,channel_func)\n\u001b[0;32m     17\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mrange\u001b[39m(input_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[92], line 60\u001b[0m, in \u001b[0;36mFiberOpticFNN0.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x2 and 1024x320)"
     ]
    }
   ],
   "source": [
    "# identity matrix --> transmitter --> identity channel --> reciever --> identity matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "input_size = 16,384\n",
    "hidden_size = 10\n",
    "channel_size = 2\n",
    "\n",
    "channel = FiberOpticFNN0(2 * window_size, params[\"hidden_dim\"], n, params[\"dropout\"])\n",
    "channel.load_state_dict(weights[\"model weights\"], strict=False)\n",
    "channel_func = lambda x: channel.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "model = TransmitterRecieverModel(input_size,hidden_size,channel_size,channel_func)\n",
    "\n",
    "input_data = np.array([range(input_size) for _ in range(1)]).reshape(-1)\n",
    "input_data_one_hot = np.eye(input_size)[input_data]\n",
    "\n",
    "model.train_model(torch.tensor(input_data_one_hot,dtype=torch.float32),torch.tensor(input_data,dtype=torch.long), learning_rate=0.1,epochs=1000)\n",
    "\n",
    "output = model.evaluate(torch.tensor(input_data_one_hot,dtype=torch.float32))\n",
    "print(\"Input Data: \")\n",
    "print(input_data)\n",
    "print(\"Output Data: \")\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 8.46257209777832\n",
      "Epoch 100 Loss: 0.33700233697891235\n",
      "Epoch 200 Loss: 0.33759158849716187\n",
      "Epoch 300 Loss: 0.01265968382358551\n",
      "Epoch 400 Loss: 0.007818239741027355\n",
      "Epoch 500 Loss: 0.00816782284528017\n",
      "Epoch 600 Loss: 0.003800460137426853\n",
      "Epoch 700 Loss: 0.0031955670565366745\n",
      "Epoch 800 Loss: 0.007572101894766092\n",
      "Epoch 900 Loss: 0.0031166505068540573\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Symbols (middle of each window):\n",
      "[ 256  768 1280 1792 2304 2816 3328 3840]\n",
      "Output Predictions:\n",
      "[ 256  768 1280 1792 2304 2816 3328 3840]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - FiberOpticFNN0, params, n, and weights are defined elsewhere.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Modified Autoencoder that groups input into windows BEFORE encoding.\n",
    "class TransmitterRecieverModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, channel_size, channel_func, window_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Dimensionality of one-hot vectors (alphabet size).\n",
    "            hidden_size (int): Hidden layer dimension.\n",
    "            channel_size (int): Dimension of the transmitter output (e.g., 2 for I/Q).\n",
    "            channel_func (callable): The fiber channel function. It expects an input of size (2 * window_length).\n",
    "            window_length (int): Number of symbols per window.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size      # e.g., 4096 (alphabet size)\n",
    "        self.window_length = window_length  # e.g., 1024\n",
    "        self.channel_size = channel_size    # e.g., 2 (I/Q)\n",
    "\n",
    "        # Transmitter: processes one symbol at a time.\n",
    "        self.transmitter_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.transmitter_fc2 = nn.Linear(hidden_size, channel_size)\n",
    "\n",
    "        # Receiver: decodes the channel output back to logits over the alphabet.\n",
    "        self.reciever_fc1 = nn.Linear(channel_size, hidden_size)\n",
    "        self.reciever_fc2 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        self.channel_function = channel_func\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (total_length, input_size), where total_length is divisible by window_length.\n",
    "        Returns:\n",
    "            A tensor of shape (num_windows, input_size), where each row corresponds to the receiver's\n",
    "            output for that window.\n",
    "        \"\"\"\n",
    "        # Group input into non-overlapping windows of size window_length.\n",
    "        num_windows = x.shape[0] // self.window_length\n",
    "        # New shape: (num_windows, window_length, input_size)\n",
    "        x_windows = x.view(num_windows, self.window_length, self.input_size)\n",
    "        \n",
    "        # Process each symbol in each window through the transmitter.\n",
    "        x_flat = x_windows.view(-1, self.input_size)  # Shape: (num_windows * window_length, input_size)\n",
    "        tx_hidden = torch.sigmoid(self.transmitter_fc1(x_flat))\n",
    "        tx_symbols = self.transmitter_fc2(tx_hidden)     # Shape: (num_windows * window_length, channel_size)\n",
    "        \n",
    "        # Reshape back into windows: (num_windows, window_length, channel_size)\n",
    "        tx_windows = tx_symbols.view(num_windows, self.window_length, self.channel_size)\n",
    "        \n",
    "        # Flatten each window to create a vector of size (window_length * channel_size).\n",
    "        # With window_length=1024 and channel_size=2, this gives a vector of size 2048.\n",
    "        tx_windows_flat = tx_windows.view(num_windows, -1)\n",
    "        \n",
    "        # Pass each flattened window through the fiber channel.\n",
    "        channel_out = self.channel_function(tx_windows_flat)  # Expected shape: (num_windows, channel_size)\n",
    "        \n",
    "        # Decode the channel output using the receiver.\n",
    "        rx_hidden = torch.sigmoid(self.reciever_fc1(channel_out))\n",
    "        rx_output = self.reciever_fc2(rx_hidden)  # Shape: (num_windows, input_size)\n",
    "        return rx_output\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            output_soft = F.softmax(output, dim=-1)\n",
    "            preds = torch.argmax(output_soft, dim=-1)\n",
    "            return preds\n",
    "\n",
    "    def train_model(self, x, y, epochs=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (total_length, input_size)\n",
    "            y: Tensor of shape (num_windows,) with the target symbol (class index) for each window.\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.forward(x)  # Shape: (num_windows, input_size)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "\n",
    "        # Plot the loss over epochs\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(loss_history, label=\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        return loss_history\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Parameters for this example:\n",
    "alphabet_size = 4096    # One-hot vector dimension; using 4096 symbols.\n",
    "hidden_size = 10\n",
    "channel_size = 2        # I/Q components.\n",
    "window_length = 512    # Each window has 1024 symbols.\n",
    "total_length = 4096     # Total number of symbols (4096/1024 = 4 windows).\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Set up the fiber channel.\n",
    "# The channel expects a flattened input of size 2 * window_length = 2 * 1024 = 2048.\n",
    "channel = FiberOpticFNN0(2 * window_length, params[\"hidden_dim\"], n, params[\"dropout\"])\n",
    "channel.load_state_dict(weights[\"model weights\"], strict=False)\n",
    "channel_func = lambda x: channel.forward(x)\n",
    "\n",
    "# Instantiate the autoencoder model.\n",
    "model = TransmitterRecieverModel(\n",
    "    input_size=alphabet_size, \n",
    "    hidden_size=hidden_size, \n",
    "    channel_size=channel_size, \n",
    "    channel_func=channel_func, \n",
    "    window_length=window_length\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate input data.\n",
    "# Here we generate a sequence of 4096 symbols (0, 1, 2, ..., 4095).\n",
    "input_data = np.array(range(total_length))  # Shape: (4096,)\n",
    "# One-hot encode each symbol: shape (4096, 4096)\n",
    "input_data_one_hot = np.eye(alphabet_size)[input_data]\n",
    "\n",
    "# Group the input into non-overlapping windows for target calculation.\n",
    "# Reshape into (num_windows, window_length).\n",
    "input_data_windows = input_data.reshape(-1, window_length)\n",
    "# For each window, choose the middle symbol (index window_length // 2) as the target.\n",
    "target_data = input_data_windows[:, window_length // 2]  # Shape: (num_windows,)\n",
    "\n",
    "# Convert data to PyTorch tensors.\n",
    "input_tensor = torch.tensor(input_data_one_hot, dtype=torch.float32)  # Shape: (4096, 4096)\n",
    "target_tensor = torch.tensor(target_data, dtype=torch.long)           # Shape: (4096 / 1024,)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train the model and plot loss.\n",
    "loss_history = model.train_model(input_tensor, target_tensor, learning_rate=0.1, epochs=1000)\n",
    "\n",
    "# Evaluate the model.\n",
    "output = model.evaluate(input_tensor)\n",
    "print(\"Target Symbols (middle of each window):\")\n",
    "print(target_tensor.cpu().numpy())\n",
    "print(\"Output Predictions:\")\n",
    "print(output.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
