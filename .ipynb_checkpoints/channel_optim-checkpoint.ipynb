{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finding the best Channel Model for an Optical Fiber Channel </h2>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Setup: imports <h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import os\n",
    "print(os.getcwd())\n",
    "path = \"model results/basic/QPSK/best_params.json\"\n",
    "\n",
    "# Ensure the parent directory exists\n",
    "#os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Example data to save\n",
    "best_params = {\"param1\": 0.1, \"param2\": 0.2}\n",
    "\n",
    "# Attempt to write the file\n",
    "try:\n",
    "    with open(path, \"w\") as json_file:\n",
    "        json.dump(best_params, json_file, indent=4)\n",
    "        print(f\"JSON file successfully saved to: {os.path.abspath(path)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while saving JSON file: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 1 - Obtain fraction of the dataset and split it into train,test, and validation subsets </h3>\n",
    "We obtain a fraction, because the original dataset size is huge and would take too long if we used all of it. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to obtain training, validation, and testing datasets\n",
    "def obtain_datasets(fraction):\n",
    "    # Helper function to load a fraction of the dataset\n",
    "    def load_fraction(file_path, fraction):\n",
    "        # Load the entire dataset\n",
    "        full_data = pd.read_csv(file_path, sep = r'\\s+', header=None, names=[\"Time\", \"Amplitude\"]).to_numpy()\n",
    "        \n",
    "        # Calculate the number of rows to load\n",
    "        num_rows = int(len(full_data) * fraction)\n",
    "        \n",
    "        # Select the first `num_rows` rows (contiguous block)\n",
    "        sampled_data = full_data[:num_rows]\n",
    "        \n",
    "        return sampled_data\n",
    "\n",
    "    # Load sub-versions of each dataset\n",
    "    pam_input_data = load_fraction(\"data/PAM-4 Input Data.txt\", fraction)\n",
    "    pam_output_data = load_fraction(\"data/PAM-4 Output Data.txt\", fraction)\n",
    "\n",
    "    qpsk_input_data = load_fraction(\"data/QPSK Input Data.txt\", fraction)\n",
    "    qpsk_output_data = load_fraction(\"data/QPSK Output Data.txt\", fraction)\n",
    "\n",
    "    qam_input_data = load_fraction(\"data/16-QAM Input Data.txt\", fraction)\n",
    "    qam_output_data = load_fraction(\"data/16-QAM Output Data.txt\", fraction)\n",
    "\n",
    "    # Helper function to split into train, validation, and test sets\n",
    "    def obtain_train_validate_test(data):\n",
    "        # Compute sizes\n",
    "        train_size = int(0.7 * len(data))\n",
    "        val_size = int(0.15 * len(data))\n",
    "        test_size = len(data) - train_size - val_size\n",
    "\n",
    "        # Perform splits\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:train_size + val_size]\n",
    "        test_data = data[train_size + val_size:]\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    # Split PAM datasets\n",
    "    pam_train_input, pam_val_input, pam_test_input = obtain_train_validate_test(pam_input_data)\n",
    "    pam_train_output, pam_val_output, pam_test_output = obtain_train_validate_test(pam_output_data)\n",
    "\n",
    "    # Split QPSK datasets\n",
    "    qpsk_train_input, qpsk_val_input, qpsk_test_input = obtain_train_validate_test(qpsk_input_data)\n",
    "    qpsk_train_output, qpsk_val_output, qpsk_test_output = obtain_train_validate_test(qpsk_output_data)\n",
    "\n",
    "    # Split QAM datasets\n",
    "    qam_train_input, qam_val_input, qam_test_input = obtain_train_validate_test(qam_input_data)\n",
    "    qam_train_output, qam_val_output, qam_test_output = obtain_train_validate_test(qam_output_data)\n",
    "\n",
    "    # Organize everything into a dictionary for easy access\n",
    "    datasets = {\n",
    "        \"PAM\": {\n",
    "            \"train\": (pam_train_input, pam_train_output),\n",
    "            \"validate\": (pam_val_input, pam_val_output),\n",
    "            \"test\": (pam_test_input, pam_test_output)\n",
    "        },\n",
    "        \"QPSK\": {\n",
    "            \"train\": (qpsk_train_input, qpsk_train_output),\n",
    "            \"validate\": (qpsk_val_input, qpsk_val_output),\n",
    "            \"test\": (qpsk_test_input, qpsk_test_output)\n",
    "        },\n",
    "        \"QAM\": {\n",
    "            \"train\": (qam_train_input, qam_train_output),\n",
    "            \"validate\": (qam_val_input, qam_val_output),\n",
    "            \"test\": (qam_test_input, qam_test_output)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return datasets\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Specify the fraction of data to use (e.g., 0.1 for 10%)\n",
    "fraction = 1\n",
    "\n",
    "# Get the datasets\n",
    "datasets = obtain_datasets(fraction)\n",
    "\n",
    "# Access the PAM training dataset\n",
    "pam_train_input, pam_train_output = datasets[\"PAM\"][\"train\"]\n",
    "\n",
    "# Access the QPSK validation dataset\n",
    "qpsk_val_input, qpsk_val_output = datasets[\"QPSK\"][\"validate\"]\n",
    "\n",
    "# Access the QAM test dataset\n",
    "qam_test_input, qam_test_output = datasets[\"QAM\"][\"test\"]\n",
    "\n",
    "print(\"PAM Training Input Shape:\", pam_train_input.shape)\n",
    "print(\"PAM Training Output Shape:\", pam_train_output.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PAM training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Input data plot\n",
    "plt.plot(qpsk_val_input[:, 0], qpsk_val_input[:, 1], label=\"Input Data\", color=\"blue\", alpha=0.7)\n",
    "\n",
    "# Output data plot\n",
    "plt.plot(qpsk_val_output[:, 0], qpsk_val_output[:, 1], label=\"Output Data\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title(\"QPSK Testing Data: Input vs. Output\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2 - Data Processing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_paths = [\"data/QPSK Input Data.txt\"]\n",
    "output_paths = [\"data/QPSK Output Data.txt\"]\n",
    "\n",
    "#initialize lists\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for i in range(len(input_paths)):\n",
    "\n",
    "    input_data_temp = pd.read_csv(input_paths[i], sep = r'\\s+', header = None, names = [\"Time\", \"Amplitude\"]).to_numpy()\n",
    "    output_data_temp = pd.read_csv(output_paths[i], sep = r'\\s+', header = None, names = [\"Time\", \"Amplitude\"]).to_numpy()\n",
    "\n",
    "    # Align data sizes by truncating to the minimum length\n",
    "    if len(input_data_temp) != len(output_data_temp):\n",
    "        min_length = min(len(input_data_temp), len(output_data_temp))\n",
    "        input_data_temp = input_data_temp[:min_length]\n",
    "        output_data_temp = output_data_temp[:min_length]\n",
    "\n",
    "    # Replace NaN values with the mean of the column\n",
    "    if np.isnan(input_data_temp[:, 1]).any():# Check for NaN values\n",
    "        input_mean = np.nanmean(input_data_temp[:, 1])  # Mean ignoring NaNs\n",
    "        input_data_temp[np.isnan(input_data_temp[:, 1]), 1] = input_mean\n",
    "\n",
    "    if np.isnan(output_data_temp[:, 1]).any():  # Check for NaN values\n",
    "        output_mean = np.nanmean(output_data_temp[:, 1])  # Mean ignoring NaNs\n",
    "        output_data_temp[np.isnan(output_data_temp[:, 1]), 1] = output_mean\n",
    "\n",
    "    #append arrays to list\n",
    "    input_data.append(input_data_temp)\n",
    "    output_data.append(output_data_temp)\n",
    "\n",
    "#convert lists to numpy arrays\n",
    "input_data = np.vstack(input_data)\n",
    "output_data = np.vstack(output_data)\n",
    "\n",
    "print(f\"Input data shape: {input_data.shape}\")\n",
    "print(f\"Output data shape: {output_data.shape}\")\n",
    "\n",
    "# Validate raw data\n",
    "print(f\"Number of datasets: {len(input_data)}\")\n",
    "print(f\"First dataset input shape: {input_data[0].shape}\")\n",
    "print(f\"First dataset output shape: {output_data[0].shape}\")\n",
    "\n",
    "# Standardize input and output data\n",
    "input_mean, input_std = input_data[:, 1].mean(), input_data[:, 1].std()\n",
    "output_mean, output_std = output_data[:, 1].mean(), output_data[:, 1].std()\n",
    "\n",
    "input_data[:, 1] = (input_data[:, 1] - input_mean) / input_std\n",
    "output_data[:, 1] = (output_data[:, 1] - output_mean) / output_std\n",
    "\n",
    "# Validate Standardized data\n",
    "print(\"\\nStandardized Input Data (First 5 rows):\")\n",
    "print(input_data[:5])\n",
    "print(\"Standardized Output Data (First 5 rows):\")\n",
    "print(output_data[:5])\n",
    "\n",
    "data = np.column_stack((input_data[:, 0], input_data[:, 1], output_data[:, 1]))\n",
    "\n",
    "# Validate combined data\n",
    "print(\"\\nCombined Data (First 5 rows):\")\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "# Sliding window function\n",
    "def create_windows(data, window_size, step_size):\n",
    "    num_windows = (len(data) - window_size) // step_size + 1\n",
    "    windows = np.array([\n",
    "        data[i:i + window_size]  # Extract rows for each window\n",
    "        for i in range(0, num_windows * step_size, step_size)\n",
    "    ])\n",
    "    return windows\n",
    "\n",
    "\n",
    "# Define window size and step size\n",
    "window_size = 512\n",
    "step_size = 1\n",
    "middle_index = window_size // 2\n",
    "windows = create_windows(data, window_size, step_size)\n",
    "\n",
    "# Validate sliding windows\n",
    "print(\"\\nSliding Windows (Shape):\", windows.shape)\n",
    "print(\"First Sliding Window (First 5 rows):\")\n",
    "print(windows[0][:5])\n",
    "\n",
    "# Flatten the input windows for FNN\n",
    "X = windows[:, :, 1].reshape(windows.shape[0], -1)  # Amplitude only\n",
    "y = windows[:, middle_index, 2]  # Output amplitude for the middle time step\n",
    "time_index = windows[:, :, 0]  # Time values retained for indexing\n",
    "\n",
    "# Validate flattened input and output\n",
    "print(\"\\nFlattened Input X (Shape):\", X.shape)\n",
    "print(\"First Flattened Input X (First 5 values):\")\n",
    "print(X[0][:5])\n",
    "print(\"\\nOutput y (Shape):\", y.shape)\n",
    "print(\"First 5 Output Values y:\")\n",
    "print(y[:5])\n",
    "print(\"\\nTime Index (Shape):\", time_index.shape)\n",
    "print(\"First Time Index (First 5 rows):\")\n",
    "print(time_index[:5])\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train, y_train, time_train = X[:train_size], y[:train_size], time_index[:train_size]\n",
    "X_val, y_val, time_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size], time_index[train_size:train_size + val_size]\n",
    "X_test, y_test, time_test = X[train_size + val_size:], y[train_size + val_size:], time_index[train_size + val_size:]\n",
    "\n",
    "# Validate splits\n",
    "print(\"\\nTraining Set Shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"\\nValidation Set Shapes:\")\n",
    "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"\\nTest Set Shapes:\")\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Validate tensor shapes\n",
    "print(\"\\nTensor Shapes:\")\n",
    "print(\"X_train_tensor:\", X_train_tensor.shape, \"y_train_tensor:\", y_train_tensor.shape)\n",
    "print(\"X_val_tensor:\", X_val_tensor.shape, \"y_val_tensor:\", y_val_tensor.shape)\n",
    "print(\"X_test_tensor:\", X_test_tensor.shape, \"y_test_tensor:\", y_test_tensor.shape)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers= 4,shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers= 4,shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers= 4,shuffle=False, pin_memory=True)\n",
    "\n",
    "# Validate DataLoader\n",
    "print(\"\\nDataLoader Validation:\")\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1} - X_batch Shape: {X_batch.shape}, y_batch Shape: {y_batch.shape}\")\n",
    "    break  # Print only the first batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Step 3 - NN models </h3>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 1: Basic Model </h4>\n",
    "\n",
    "- This is a shallow neural network serving as a baseline for fiber optic data regression tasks.\n",
    "- Features two hidden layers with progressive dimensionality reduction (hidden_dim to hidden_dim // 2).\n",
    "- Incorporates dropout layers and batch normalization for regularization and training stability.\n",
    "- Prioritizes simplicity and computational efficiency.\n",
    "- Acts as a benchmark for performance comparison with other architectures.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the basic FNN model\n",
    "class FiberOpticFNN0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN0, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),  # Dropout for regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 2: Deeper model </h4>\n",
    "\n",
    "- This is a deeper neural network designed to enhance modeling capacity for fiber optic data regression.\n",
    "- Consists of three hidden layers, maintaining hidden_dim for two layers before reducing to hidden_dim // 2.\n",
    "- Employs dropout layers and batch normalization to mitigate overfitting and improve training stability.\n",
    "- Leverages additional depth to capture complex patterns and nuances in data.\n",
    "- Serves as an extended architecture to evaluate the benefits of increased depth over simpler models\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the deeper model\n",
    "class FiberOpticFNN1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 3: Wider Model </h4>\n",
    "\n",
    "- This is a wider neural network architecture\n",
    "- Begins with a significantly wider first hidden layer (hidden_dim * 2) to enhance feature extraction capacity.\n",
    "- Progressively narrows through subsequent layers, reducing to hidden_dim and then hidden_dim // 2.\n",
    "- Incorporates batch normalization and dropout layers to improve training stability and mitigate overfitting.\n",
    "- Aims to assess the benefits of increased layer width in capturing complex data patterns compared to deeper or simpler models.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the wider model\n",
    "class FiberOpticFNN2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 4: Dynamic Model </h4>\n",
    "\n",
    "- This is a dynamic neural network architecture\n",
    "- Features a progressively shrinking hidden layer size, starting from hidden_dim and reducing by a factor of 0.75 and 0.5 in subsequent layers.\n",
    "- Employs batch normalization after each layer to stabilize training and dropout to reduce overfitting.\n",
    "- Incorporates ReLU activations for non-linearity and efficient feature learning.\n",
    "- Evaluates the effectiveness of a dynamically shrinking architecture in balancing complexity and computational efficiency.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the dynamic model\n",
    "class FiberOpticFNN3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN3, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, int(hidden_dim * 0.75)),\n",
    "            nn.BatchNorm1d(int(hidden_dim * 0.75)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(hidden_dim * 0.75), int(hidden_dim * 0.5)),\n",
    "            nn.BatchNorm1d(int(hidden_dim * 0.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(hidden_dim * 0.5), output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 5: Noise Resilient Model </h4>\n",
    "\n",
    "- This is explicitly designed to be noise-resilient, making it adept at capturing small but frequent deviations in fiber optic data, which may appear noise-like.\n",
    "- The model begins with a feature extractor, which maps the input into a higher-dimensional representation. This step ensures that the core characteristics of the data are well-represented for further processing. The use of batch normalization and ReLU activation enhances stability and non-linearity.\n",
    "- A specialized noise-focused branch is incorporated to target and capture subtle variations in the data. By reducing the feature dimension and employing a Tanh activation function, this branch emphasizes small deviations while minimizing overfitting with dropout.\n",
    "- A residual pathway is added to preserve the original feature representation from the extractor. This helps the model maintain key input information while refining the features for the final prediction.\n",
    "- The model combines features from the noise-sensitive branch and the residual pathway through concatenation, creating a rich feature set for prediction. This enables the model to balance sensitivity to noise with robustness in prediction.\n",
    "- The combined features are passed through a fully connected layer with batch normalization and ReLU activation before producing the final output. This ensures effective learning of complex patterns while maintaining regularization.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the noise-resilient model\n",
    "class FiberOpticFNN4(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FiberOpticFNN4, self).__init__()\n",
    "\n",
    "        # Initial feature extraction\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Noise-focused branch (captures small deviations)\n",
    "        self.noise_branch = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Residual connection for refined outputs\n",
    "        self.residual = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Final layer combining noise and refined features\n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Process noise-sensitive features\n",
    "        noise_features = self.noise_branch(features)\n",
    "\n",
    "        # Add residual connection\n",
    "        refined_features = features + self.residual(features)\n",
    "\n",
    "        # Combine noise-sensitive and refined features\n",
    "        combined_input = torch.cat((refined_features, noise_features), dim=1)\n",
    "\n",
    "        # Final output\n",
    "        output = self.combined(combined_input)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4> model 6: Residual Connection Model </h4>\n",
    "\n",
    "- This is a neural network model designed to improve the learning process using residual connections.\n",
    "- Starts with a hidden layer that applies batch normalization and ReLU activation for non-linearity, allowing the model to capture complex patterns.\n",
    "- Introduces a residual connection between the input and hidden layer, enabling the model to retain original features while learning refined representations, which helps prevent vanishing gradients and enhances training stability.\n",
    "- The output layer generates predictions by mapping the hidden features to the target output dimension.\n",
    "- Evaluates the effectiveness of residual connections in maintaining feature integrity while refining complex data representations, particularly useful for fiber optic signal processing tasks.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the Residual Connections model\n",
    "class FiberOpticFNN5(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FiberOpticFNN5, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = x + self.hidden_layer(x)  # Residual connection\n",
    "        return self.output_layer(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Step 4 - Training loop </h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_weights = None\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            # Move data to GPU\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                # Move data to GPU\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            patience_counter = 0\n",
    "            best_weights = {\"model weights\": model.state_dict()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, best_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Step 5 - Define Objective functions </h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Basic\n",
    "def objective0(trial):\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Ensure directory exists\n",
    "    base_dir = \"model results/basic/QPSK\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step=16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = FiberOpticFNN0(X_train.shape[1], hidden_dim, 1, dropout_rate)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss()  # Define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # Define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "\n",
    "        # Save model weights\n",
    "        weights_path = os.path.join(base_dir, \"model_weights.pth\")\n",
    "        torch.save(best_weights, weights_path)\n",
    "\n",
    "        # Save best params\n",
    "        best_params = {\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses\n",
    "        }\n",
    "        params_path = os.path.join(base_dir, \"best_params.json\")\n",
    "        with open(params_path, \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "        print(f\"Files saved successfully: {weights_path}, {params_path}\")\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "#deeper\n",
    "def objective1(trial):\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step= 16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log =True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    #instatiate the model\n",
    "    model = FiberOpticFNN1(X_train.shape[1], hidden_dim, 1, dropout_rate)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss() #define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "        torch.save(best_weights, \"model results/deeper/QPSK/model_weights.pth\")\n",
    "        best_params = {\"hidden_dim\": hidden_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"dropout_rate\": dropout_rate,\n",
    "                       \"train_losses\": train_losses,\n",
    "                       \"val_losses\": val_losses\n",
    "                       }\n",
    "        with open(\"model results/deeper/QPSK/best_params.json\", \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Wider\n",
    "def objective2(trial):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step= 16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log =True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    #instatiate the model\n",
    "    model = FiberOpticFNN2(X_train.shape[1], hidden_dim, 1, dropout_rate)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss() #define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "        torch.save(best_weights, \"model results/wider/QPSK/model_weights.pth\")\n",
    "        best_params = {\"hidden_dim\": hidden_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"dropout_rate\": dropout_rate,\n",
    "                       \"train_losses\": train_losses,\n",
    "                       \"val_losses\": val_losses\n",
    "                       }\n",
    "        with open(\"model results/wider/QPSK/best_params.json\", \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Dynamic\n",
    "def objective3(trial):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step= 16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log =True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    #instatiate the model\n",
    "    model = FiberOpticFNN3(X_train.shape[1], hidden_dim, 1, dropout_rate)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss() #define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "        torch.save(best_weights, \"model results/dynamic/QPSK/model_weights.pth\")\n",
    "        best_params = {\"hidden_dim\": hidden_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"dropout_rate\": dropout_rate,\n",
    "                       \"train_losses\": train_losses,\n",
    "                       \"val_losses\": val_losses\n",
    "                       }\n",
    "        with open(\"model results/dynamic/QPSK/best_params.json\", \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# noise resilient\n",
    "def objective4(trial):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step= 16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log =True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    #instatiate the model\n",
    "    model = FiberOpticFNN4(X_train.shape[1], hidden_dim, 1, dropout_rate)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss() #define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "        torch.save(best_weights, \"model results/NR/QPSK/model_weights.pth\")\n",
    "        best_params = {\"hidden_dim\": hidden_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"dropout_rate\": dropout_rate,\n",
    "                       \"train_losses\": train_losses,\n",
    "                       \"val_losses\": val_losses\n",
    "                       }\n",
    "        with open(\"model results/NR/QPSK/best_params.json\", \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "#Residual Connections\n",
    "def objective5(trial):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden dim\", 128, 320, step= 16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight decay\", 1e-6, 1e-3, log =True)\n",
    "    #dropout_rate = trial.suggest_float(\"dropout rate\", 0.1, 0.5)\n",
    "\n",
    "    #instatiate the model\n",
    "    model = FiberOpticFNN5(X_train.shape[1], hidden_dim, 1)\n",
    "    model = model.float()\n",
    "\n",
    "    criterion = nn.MSELoss() #define loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #define optimizer\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_weights = train_model(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "    if min(val_losses) < best_val_loss:\n",
    "        best_val_loss = min(val_losses)\n",
    "        torch.save(best_weights, \"model results/residual/QPSK/model_weights.pth\")\n",
    "        best_params = {\"hidden_dim\": hidden_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       #\"dropout_rate\": dropout_rate,\n",
    "                       \"train_losses\": train_losses,\n",
    "                       \"val_losses\": val_losses\n",
    "                       }\n",
    "        with open(\"model results/residual/QPSK/best_params.json\", \"w\") as json_file:\n",
    "            json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "    return best_val_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Step 6 - Run the studies </h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Create a studies for hyperparameter optimization\n",
    "study0 = optuna.create_study(direction=\"minimize\")  # Minimize the validation loss\n",
    "study0.optimize(objective0, n_trials=100)   #Run 100 trials"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "study1 = optuna.create_study(direction=\"minimize\")\n",
    "study1.optimize(objective1, n_trials=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "study2 = optuna.create_study(direction=\"minimize\")\n",
    "study2.optimize(objective2, n_trials=100)\n",
    "\n",
    "study3 = optuna.create_study(direction=\"minimize\")\n",
    "study3.optimize(objective3, n_trials=100)\n",
    "\n",
    "study4 = optuna.create_study(direction=\"minimize\")\n",
    "study4.optimize(objective4, n_trials=100)\n",
    "\n",
    "study5 = optuna.create_study(direction=\"minimize\")\n",
    "study5.optimize(objective5, n_trials=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.8029, Val Loss: 1.0514\n",
      "Epoch 11/100, Train Loss: 0.7905, Val Loss: 1.0509\n",
      "Epoch 12/100, Train Loss: 0.7791, Val Loss: 1.0683\n",
      "Epoch 13/100, Train Loss: 0.7680, Val Loss: 1.0774\n",
      "Epoch 14/100, Train Loss: 0.7561, Val Loss: 1.0702\n",
      "Epoch 15/100, Train Loss: 0.7452, Val Loss: 1.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 15:05:00,510] Trial 10 finished with value: 0.9951167145332733 and parameters: {'hidden dim': 320, 'lr': 6.392957632566066e-05, 'weight decay': 8.09785188900409e-06}. Best is trial 7 with value: 0.9894521944708639.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.7351, Val Loss: 1.0835\n",
      "Early stopping triggered\n",
      "Epoch 1/100, Train Loss: 0.9547, Val Loss: 0.9938\n",
      "Epoch 2/100, Train Loss: 0.9146, Val Loss: 1.0010\n",
      "Epoch 3/100, Train Loss: 0.8985, Val Loss: 1.0004\n",
      "Epoch 4/100, Train Loss: 0.8848, Val Loss: 1.0062\n",
      "Epoch 5/100, Train Loss: 0.8712, Val Loss: 1.0123\n",
      "Epoch 6/100, Train Loss: 0.8586, Val Loss: 1.0159\n",
      "Epoch 7/100, Train Loss: 0.8464, Val Loss: 1.0269\n",
      "Epoch 8/100, Train Loss: 0.8344, Val Loss: 1.0304\n",
      "Epoch 9/100, Train Loss: 0.8236, Val Loss: 1.0401\n",
      "Epoch 10/100, Train Loss: 0.8119, Val Loss: 1.0453\n",
      "Epoch 11/100, Train Loss: 0.8006, Val Loss: 1.0571\n",
      "Epoch 12/100, Train Loss: 0.7904, Val Loss: 1.0567\n",
      "Epoch 13/100, Train Loss: 0.7794, Val Loss: 1.0777\n",
      "Epoch 14/100, Train Loss: 0.7704, Val Loss: 1.0850\n",
      "Epoch 15/100, Train Loss: 0.7599, Val Loss: 1.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 15:06:54,125] Trial 11 finished with value: 0.993777049439294 and parameters: {'hidden dim': 272, 'lr': 6.775026468049846e-05, 'weight decay': 8.073469108958549e-06}. Best is trial 7 with value: 0.9894521944708639.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.7509, Val Loss: 1.0851\n",
      "Early stopping triggered\n",
      "Epoch 1/100, Train Loss: 0.9650, Val Loss: 0.9960\n",
      "Epoch 2/100, Train Loss: 0.9140, Val Loss: 1.0002\n",
      "Epoch 3/100, Train Loss: 0.8975, Val Loss: 1.0026\n",
      "Epoch 4/100, Train Loss: 0.8835, Val Loss: 1.0131\n",
      "Epoch 5/100, Train Loss: 0.8705, Val Loss: 1.0179\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
